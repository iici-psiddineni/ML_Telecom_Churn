{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Telecom Churn Case Study</font>\n",
    "* Institution: IIIT, Bangalore and UpGrad\n",
    "* Course: PG Diploma in Machine Lerning and AI March 2018\n",
    "* Date: 14-Aug-2018\n",
    "* Submitted by:\n",
    "    1. Pandinath Siddineni (ID- APFE187000194)\n",
    "    2. AKNR Chandra Sekhar (ID- APFE187000315)\n",
    "    3. Brajesh Kumar       (ID- APFE187000149)\n",
    "    4. Shweta Tiwari\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>PART 3: FEATURE REDUCTION USING PCA</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean telecom data file\n",
    "master_df = pd.read_csv('E:\\IIIT Bangalore AIML\\Group Assignment 2\\\\telecom_churn_data_clean2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>onnet_mou_6</th>\n",
       "      <th>onnet_mou_7</th>\n",
       "      <th>onnet_mou_8</th>\n",
       "      <th>offnet_mou_6</th>\n",
       "      <th>offnet_mou_7</th>\n",
       "      <th>offnet_mou_8</th>\n",
       "      <th>roam_ic_mou_6</th>\n",
       "      <th>roam_ic_mou_7</th>\n",
       "      <th>roam_ic_mou_8</th>\n",
       "      <th>...</th>\n",
       "      <th>rech_days_left_data_7</th>\n",
       "      <th>rech_days_left_8</th>\n",
       "      <th>rech_days_left_data_8</th>\n",
       "      <th>churn</th>\n",
       "      <th>night_pck_churn_6</th>\n",
       "      <th>night_pck_churn_7</th>\n",
       "      <th>night_pck_churn_8</th>\n",
       "      <th>fb_churn_6</th>\n",
       "      <th>fb_churn_7</th>\n",
       "      <th>fb_churn_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000701601</td>\n",
       "      <td>57.84</td>\n",
       "      <td>54.68</td>\n",
       "      <td>52.29</td>\n",
       "      <td>453.43</td>\n",
       "      <td>567.16</td>\n",
       "      <td>325.91</td>\n",
       "      <td>16.23</td>\n",
       "      <td>33.49</td>\n",
       "      <td>31.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7002311591</td>\n",
       "      <td>288.56</td>\n",
       "      <td>376.66</td>\n",
       "      <td>111.61</td>\n",
       "      <td>186.59</td>\n",
       "      <td>1326.06</td>\n",
       "      <td>771.14</td>\n",
       "      <td>52.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000959346</td>\n",
       "      <td>120.19</td>\n",
       "      <td>236.14</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2082.18</td>\n",
       "      <td>2532.03</td>\n",
       "      <td>408.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000104470</td>\n",
       "      <td>1241.99</td>\n",
       "      <td>1026.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>112.91</td>\n",
       "      <td>115.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000409785</td>\n",
       "      <td>424.98</td>\n",
       "      <td>328.73</td>\n",
       "      <td>363.98</td>\n",
       "      <td>457.09</td>\n",
       "      <td>361.34</td>\n",
       "      <td>391.68</td>\n",
       "      <td>315.29</td>\n",
       "      <td>303.04</td>\n",
       "      <td>254.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mobile_number  onnet_mou_6  onnet_mou_7  onnet_mou_8  offnet_mou_6  \\\n",
       "0     7000701601        57.84        54.68        52.29        453.43   \n",
       "1     7002311591       288.56       376.66       111.61        186.59   \n",
       "2     7000959346       120.19       236.14         1.71       2082.18   \n",
       "3     7000104470      1241.99      1026.66         0.00        112.91   \n",
       "4     7000409785       424.98       328.73       363.98        457.09   \n",
       "\n",
       "   offnet_mou_7  offnet_mou_8  roam_ic_mou_6  roam_ic_mou_7  roam_ic_mou_8  \\\n",
       "0        567.16        325.91          16.23          33.49          31.64   \n",
       "1       1326.06        771.14          52.96           0.00           7.28   \n",
       "2       2532.03        408.54           0.00           0.00           0.00   \n",
       "3        115.13          0.00           0.00           0.00           0.38   \n",
       "4        361.34        391.68         315.29         303.04         254.34   \n",
       "\n",
       "      ...      rech_days_left_data_7  rech_days_left_8  rech_days_left_data_8  \\\n",
       "0     ...                       0.00              5.00                   0.00   \n",
       "1     ...                       0.00              6.00                   0.00   \n",
       "2     ...                       0.00              3.00                   0.00   \n",
       "3     ...                       0.00              2.00                   0.00   \n",
       "4     ...                       0.00              1.00                   0.00   \n",
       "\n",
       "   churn  night_pck_churn_6  night_pck_churn_7  night_pck_churn_8  fb_churn_6  \\\n",
       "0      1               0.07               0.08               0.09        0.07   \n",
       "1      1               0.07               0.08               0.09        0.07   \n",
       "2      1               0.07               0.08               0.09        0.07   \n",
       "3      1               0.07               0.08               0.09        0.07   \n",
       "4      1               0.07               0.08               0.09        0.07   \n",
       "\n",
       "   fb_churn_7  fb_churn_8  \n",
       "0        0.08        0.09  \n",
       "1        0.08        0.09  \n",
       "2        0.08        0.09  \n",
       "3        0.08        0.09  \n",
       "4        0.08        0.09  \n",
       "\n",
       "[5 rows x 144 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobile_number',\n",
       " 'onnet_mou_6',\n",
       " 'onnet_mou_7',\n",
       " 'onnet_mou_8',\n",
       " 'offnet_mou_6',\n",
       " 'offnet_mou_7',\n",
       " 'offnet_mou_8',\n",
       " 'roam_ic_mou_6',\n",
       " 'roam_ic_mou_7',\n",
       " 'roam_ic_mou_8',\n",
       " 'roam_og_mou_6',\n",
       " 'roam_og_mou_7',\n",
       " 'roam_og_mou_8',\n",
       " 'loc_og_t2t_mou_6',\n",
       " 'loc_og_t2t_mou_7',\n",
       " 'loc_og_t2t_mou_8',\n",
       " 'loc_og_t2m_mou_6',\n",
       " 'loc_og_t2m_mou_7',\n",
       " 'loc_og_t2m_mou_8',\n",
       " 'loc_og_t2f_mou_6',\n",
       " 'loc_og_t2f_mou_7',\n",
       " 'loc_og_t2f_mou_8',\n",
       " 'loc_og_t2c_mou_6',\n",
       " 'loc_og_t2c_mou_7',\n",
       " 'loc_og_t2c_mou_8',\n",
       " 'loc_og_mou_6',\n",
       " 'loc_og_mou_7',\n",
       " 'loc_og_mou_8',\n",
       " 'std_og_t2t_mou_6',\n",
       " 'std_og_t2t_mou_7',\n",
       " 'std_og_t2t_mou_8',\n",
       " 'std_og_t2m_mou_6',\n",
       " 'std_og_t2m_mou_7',\n",
       " 'std_og_t2m_mou_8',\n",
       " 'std_og_t2f_mou_6',\n",
       " 'std_og_t2f_mou_7',\n",
       " 'std_og_t2f_mou_8',\n",
       " 'std_og_mou_6',\n",
       " 'std_og_mou_7',\n",
       " 'std_og_mou_8',\n",
       " 'isd_og_mou_6',\n",
       " 'isd_og_mou_7',\n",
       " 'isd_og_mou_8',\n",
       " 'spl_og_mou_6',\n",
       " 'spl_og_mou_7',\n",
       " 'spl_og_mou_8',\n",
       " 'og_others_6',\n",
       " 'og_others_7',\n",
       " 'og_others_8',\n",
       " 'total_og_mou_6',\n",
       " 'total_og_mou_7',\n",
       " 'total_og_mou_8',\n",
       " 'loc_ic_t2t_mou_6',\n",
       " 'loc_ic_t2t_mou_7',\n",
       " 'loc_ic_t2t_mou_8',\n",
       " 'loc_ic_t2m_mou_6',\n",
       " 'loc_ic_t2m_mou_7',\n",
       " 'loc_ic_t2m_mou_8',\n",
       " 'loc_ic_t2f_mou_6',\n",
       " 'loc_ic_t2f_mou_7',\n",
       " 'loc_ic_t2f_mou_8',\n",
       " 'loc_ic_mou_6',\n",
       " 'loc_ic_mou_7',\n",
       " 'loc_ic_mou_8',\n",
       " 'std_ic_t2t_mou_6',\n",
       " 'std_ic_t2t_mou_7',\n",
       " 'std_ic_t2t_mou_8',\n",
       " 'std_ic_t2m_mou_6',\n",
       " 'std_ic_t2m_mou_7',\n",
       " 'std_ic_t2m_mou_8',\n",
       " 'std_ic_t2f_mou_6',\n",
       " 'std_ic_t2f_mou_7',\n",
       " 'std_ic_t2f_mou_8',\n",
       " 'std_ic_mou_6',\n",
       " 'std_ic_mou_7',\n",
       " 'std_ic_mou_8',\n",
       " 'total_ic_mou_6',\n",
       " 'total_ic_mou_7',\n",
       " 'total_ic_mou_8',\n",
       " 'spl_ic_mou_6',\n",
       " 'spl_ic_mou_7',\n",
       " 'spl_ic_mou_8',\n",
       " 'isd_ic_mou_6',\n",
       " 'isd_ic_mou_7',\n",
       " 'isd_ic_mou_8',\n",
       " 'ic_others_6',\n",
       " 'ic_others_7',\n",
       " 'ic_others_8',\n",
       " 'total_rech_amt_6',\n",
       " 'total_rech_amt_7',\n",
       " 'total_rech_amt_8',\n",
       " 'max_rech_amt_6',\n",
       " 'max_rech_amt_7',\n",
       " 'max_rech_amt_8',\n",
       " 'last_day_rch_amt_6',\n",
       " 'last_day_rch_amt_7',\n",
       " 'last_day_rch_amt_8',\n",
       " 'total_rech_data_6',\n",
       " 'total_rech_data_7',\n",
       " 'total_rech_data_8',\n",
       " 'max_rech_data_6',\n",
       " 'max_rech_data_7',\n",
       " 'max_rech_data_8',\n",
       " 'vol_2g_mb_6',\n",
       " 'vol_2g_mb_7',\n",
       " 'vol_2g_mb_8',\n",
       " 'vol_3g_mb_6',\n",
       " 'vol_3g_mb_7',\n",
       " 'vol_3g_mb_8',\n",
       " 'night_pck_user_6',\n",
       " 'night_pck_user_7',\n",
       " 'night_pck_user_8',\n",
       " 'monthly_2g_6',\n",
       " 'monthly_2g_7',\n",
       " 'monthly_2g_8',\n",
       " 'sachet_2g_6',\n",
       " 'sachet_2g_7',\n",
       " 'sachet_2g_8',\n",
       " 'monthly_3g_6',\n",
       " 'monthly_3g_7',\n",
       " 'monthly_3g_8',\n",
       " 'sachet_3g_6',\n",
       " 'sachet_3g_7',\n",
       " 'sachet_3g_8',\n",
       " 'fb_user_6',\n",
       " 'fb_user_7',\n",
       " 'fb_user_8',\n",
       " 'aon',\n",
       " 'total_rech_data_amt_6',\n",
       " 'total_rech_data_amt_7',\n",
       " 'total_rech_data_amt_8',\n",
       " 'rech_days_left_6',\n",
       " 'rech_days_left_data_6',\n",
       " 'rech_days_left_7',\n",
       " 'rech_days_left_data_7',\n",
       " 'rech_days_left_8',\n",
       " 'rech_days_left_data_8',\n",
       " 'churn',\n",
       " 'night_pck_churn_6',\n",
       " 'night_pck_churn_7',\n",
       " 'night_pck_churn_8',\n",
       " 'fb_churn_6',\n",
       " 'fb_churn_7',\n",
       " 'fb_churn_8']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28504, 144)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom = master_df.drop(['mobile_number'], 1)\n",
    "#telecom = telecom.drop(['fb_user_6', 'fb_user_7', 'fb_user_8', 'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8'], 1)\n",
    "telecom = telecom.drop(['fb_churn_6', 'fb_churn_7', 'fb_churn_8', 'night_pck_churn_6', 'night_pck_churn_7', 'night_pck_churn_8'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onnet_mou_6</th>\n",
       "      <th>onnet_mou_7</th>\n",
       "      <th>onnet_mou_8</th>\n",
       "      <th>offnet_mou_6</th>\n",
       "      <th>offnet_mou_7</th>\n",
       "      <th>offnet_mou_8</th>\n",
       "      <th>roam_ic_mou_6</th>\n",
       "      <th>roam_ic_mou_7</th>\n",
       "      <th>roam_ic_mou_8</th>\n",
       "      <th>roam_og_mou_6</th>\n",
       "      <th>...</th>\n",
       "      <th>total_rech_data_amt_6</th>\n",
       "      <th>total_rech_data_amt_7</th>\n",
       "      <th>total_rech_data_amt_8</th>\n",
       "      <th>rech_days_left_6</th>\n",
       "      <th>rech_days_left_data_6</th>\n",
       "      <th>rech_days_left_7</th>\n",
       "      <th>rech_days_left_data_7</th>\n",
       "      <th>rech_days_left_8</th>\n",
       "      <th>rech_days_left_data_8</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.84</td>\n",
       "      <td>54.68</td>\n",
       "      <td>52.29</td>\n",
       "      <td>453.43</td>\n",
       "      <td>567.16</td>\n",
       "      <td>325.91</td>\n",
       "      <td>16.23</td>\n",
       "      <td>33.49</td>\n",
       "      <td>31.64</td>\n",
       "      <td>23.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>288.56</td>\n",
       "      <td>376.66</td>\n",
       "      <td>111.61</td>\n",
       "      <td>186.59</td>\n",
       "      <td>1326.06</td>\n",
       "      <td>771.14</td>\n",
       "      <td>52.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.28</td>\n",
       "      <td>10.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120.19</td>\n",
       "      <td>236.14</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2082.18</td>\n",
       "      <td>2532.03</td>\n",
       "      <td>408.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1241.99</td>\n",
       "      <td>1026.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>112.91</td>\n",
       "      <td>115.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424.98</td>\n",
       "      <td>328.73</td>\n",
       "      <td>363.98</td>\n",
       "      <td>457.09</td>\n",
       "      <td>361.34</td>\n",
       "      <td>391.68</td>\n",
       "      <td>315.29</td>\n",
       "      <td>303.04</td>\n",
       "      <td>254.34</td>\n",
       "      <td>613.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   onnet_mou_6  onnet_mou_7  onnet_mou_8  offnet_mou_6  offnet_mou_7  \\\n",
       "0        57.84        54.68        52.29        453.43        567.16   \n",
       "1       288.56       376.66       111.61        186.59       1326.06   \n",
       "2       120.19       236.14         1.71       2082.18       2532.03   \n",
       "3      1241.99      1026.66         0.00        112.91        115.13   \n",
       "4       424.98       328.73       363.98        457.09        361.34   \n",
       "\n",
       "   offnet_mou_8  roam_ic_mou_6  roam_ic_mou_7  roam_ic_mou_8  roam_og_mou_6  \\\n",
       "0        325.91          16.23          33.49          31.64          23.74   \n",
       "1        771.14          52.96           0.00           7.28          10.69   \n",
       "2        408.54           0.00           0.00           0.00           0.00   \n",
       "3          0.00           0.00           0.00           0.38           0.00   \n",
       "4        391.68         315.29         303.04         254.34         613.84   \n",
       "\n",
       "   ...    total_rech_data_amt_6  total_rech_data_amt_7  total_rech_data_amt_8  \\\n",
       "0  ...                     0.00                   0.00                   0.00   \n",
       "1  ...                     0.00                   0.00                   0.00   \n",
       "2  ...                     0.00                   0.00                   0.00   \n",
       "3  ...                     0.00                   0.00                   0.00   \n",
       "4  ...                     0.00                   0.00                   0.00   \n",
       "\n",
       "   rech_days_left_6  rech_days_left_data_6  rech_days_left_7  \\\n",
       "0              3.00                   0.00              6.00   \n",
       "1              0.00                   0.00              4.00   \n",
       "2              0.00                   0.00              0.00   \n",
       "3              1.00                   0.00              2.00   \n",
       "4              2.00                   0.00              1.00   \n",
       "\n",
       "   rech_days_left_data_7  rech_days_left_8  rech_days_left_data_8  churn  \n",
       "0                   0.00              5.00                   0.00      1  \n",
       "1                   0.00              6.00                   0.00      1  \n",
       "2                   0.00              3.00                   0.00      1  \n",
       "3                   0.00              2.00                   0.00      1  \n",
       "4                   0.00              1.00                   0.00      1  \n",
       "\n",
       "[5 rows x 137 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "telecom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53816, 136)\n",
      "(53816,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26908"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X = telecom.drop(['churn'],axis=1)\n",
    "y = telecom['churn']\n",
    "sm = SMOTE(kind = \"regular\")\n",
    "X_tr,y_tr = sm.fit_sample(X,y)\n",
    "print(X_tr.shape)\n",
    "print(y_tr.shape)\n",
    "np.count_nonzero(y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X\n",
    "y_tr = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Aknrcsekhar\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tr,y_tr, train_size=0.7,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onnet_mou_6</th>\n",
       "      <th>onnet_mou_7</th>\n",
       "      <th>onnet_mou_8</th>\n",
       "      <th>offnet_mou_6</th>\n",
       "      <th>offnet_mou_7</th>\n",
       "      <th>offnet_mou_8</th>\n",
       "      <th>roam_ic_mou_6</th>\n",
       "      <th>roam_ic_mou_7</th>\n",
       "      <th>roam_ic_mou_8</th>\n",
       "      <th>roam_og_mou_6</th>\n",
       "      <th>...</th>\n",
       "      <th>aon</th>\n",
       "      <th>total_rech_data_amt_6</th>\n",
       "      <th>total_rech_data_amt_7</th>\n",
       "      <th>total_rech_data_amt_8</th>\n",
       "      <th>rech_days_left_6</th>\n",
       "      <th>rech_days_left_data_6</th>\n",
       "      <th>rech_days_left_7</th>\n",
       "      <th>rech_days_left_data_7</th>\n",
       "      <th>rech_days_left_8</th>\n",
       "      <th>rech_days_left_data_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11465</th>\n",
       "      <td>125.09</td>\n",
       "      <td>81.18</td>\n",
       "      <td>58.99</td>\n",
       "      <td>11.28</td>\n",
       "      <td>6.44</td>\n",
       "      <td>16.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2845</td>\n",
       "      <td>580.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>145.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21150</th>\n",
       "      <td>19.33</td>\n",
       "      <td>85.69</td>\n",
       "      <td>55.01</td>\n",
       "      <td>1002.11</td>\n",
       "      <td>1301.99</td>\n",
       "      <td>1378.78</td>\n",
       "      <td>387.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>105.79</td>\n",
       "      <td>...</td>\n",
       "      <td>2867</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>4.21</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.66</td>\n",
       "      <td>7.84</td>\n",
       "      <td>11.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>384</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2496.00</td>\n",
       "      <td>856.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13702</th>\n",
       "      <td>58.33</td>\n",
       "      <td>82.51</td>\n",
       "      <td>50.18</td>\n",
       "      <td>354.54</td>\n",
       "      <td>351.79</td>\n",
       "      <td>252.21</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>...</td>\n",
       "      <td>1122</td>\n",
       "      <td>368.00</td>\n",
       "      <td>368.00</td>\n",
       "      <td>154.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>65.14</td>\n",
       "      <td>76.81</td>\n",
       "      <td>45.06</td>\n",
       "      <td>483.84</td>\n",
       "      <td>546.34</td>\n",
       "      <td>258.24</td>\n",
       "      <td>0.66</td>\n",
       "      <td>9.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>...</td>\n",
       "      <td>3098</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5502</th>\n",
       "      <td>80.64</td>\n",
       "      <td>272.33</td>\n",
       "      <td>462.04</td>\n",
       "      <td>259.03</td>\n",
       "      <td>372.08</td>\n",
       "      <td>124.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>572</td>\n",
       "      <td>225.00</td>\n",
       "      <td>1225.00</td>\n",
       "      <td>1225.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10325</th>\n",
       "      <td>0.80</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.96</td>\n",
       "      <td>8.84</td>\n",
       "      <td>3.86</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>793</td>\n",
       "      <td>252.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>148.86</td>\n",
       "      <td>7.86</td>\n",
       "      <td>3.19</td>\n",
       "      <td>13.28</td>\n",
       "      <td>37.01</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>278</td>\n",
       "      <td>666.00</td>\n",
       "      <td>1036.00</td>\n",
       "      <td>768.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228</th>\n",
       "      <td>426.48</td>\n",
       "      <td>442.51</td>\n",
       "      <td>482.33</td>\n",
       "      <td>862.38</td>\n",
       "      <td>1072.79</td>\n",
       "      <td>868.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2576</td>\n",
       "      <td>1008.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23573</th>\n",
       "      <td>59.68</td>\n",
       "      <td>88.46</td>\n",
       "      <td>75.69</td>\n",
       "      <td>458.89</td>\n",
       "      <td>407.53</td>\n",
       "      <td>231.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3651</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>1640.00</td>\n",
       "      <td>1016.00</td>\n",
       "      <td>1215.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>186.13</td>\n",
       "      <td>81.58</td>\n",
       "      <td>42.16</td>\n",
       "      <td>599.24</td>\n",
       "      <td>255.68</td>\n",
       "      <td>49.94</td>\n",
       "      <td>20.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.83</td>\n",
       "      <td>...</td>\n",
       "      <td>858</td>\n",
       "      <td>248.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>304.36</td>\n",
       "      <td>193.26</td>\n",
       "      <td>311.19</td>\n",
       "      <td>570.19</td>\n",
       "      <td>284.49</td>\n",
       "      <td>218.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2234</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>137.36</td>\n",
       "      <td>152.41</td>\n",
       "      <td>154.33</td>\n",
       "      <td>719.11</td>\n",
       "      <td>650.31</td>\n",
       "      <td>505.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.36</td>\n",
       "      <td>7.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>363</td>\n",
       "      <td>0.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>616.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19652</th>\n",
       "      <td>749.33</td>\n",
       "      <td>584.66</td>\n",
       "      <td>975.39</td>\n",
       "      <td>996.61</td>\n",
       "      <td>789.48</td>\n",
       "      <td>422.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1104</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>9.14</td>\n",
       "      <td>14.81</td>\n",
       "      <td>30.46</td>\n",
       "      <td>63.63</td>\n",
       "      <td>182.79</td>\n",
       "      <td>254.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2088.00</td>\n",
       "      <td>2180.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11500</th>\n",
       "      <td>4.19</td>\n",
       "      <td>0.66</td>\n",
       "      <td>6.11</td>\n",
       "      <td>129.76</td>\n",
       "      <td>102.09</td>\n",
       "      <td>49.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2983</td>\n",
       "      <td>608.00</td>\n",
       "      <td>608.00</td>\n",
       "      <td>2432.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9621</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.74</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>879.00</td>\n",
       "      <td>98.00</td>\n",
       "      <td>505.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28053</th>\n",
       "      <td>24.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>174.76</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>251</td>\n",
       "      <td>1372.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25995</th>\n",
       "      <td>248.41</td>\n",
       "      <td>388.91</td>\n",
       "      <td>364.36</td>\n",
       "      <td>728.19</td>\n",
       "      <td>571.14</td>\n",
       "      <td>930.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>422</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22380</th>\n",
       "      <td>161.39</td>\n",
       "      <td>1126.93</td>\n",
       "      <td>805.14</td>\n",
       "      <td>41.68</td>\n",
       "      <td>114.79</td>\n",
       "      <td>52.09</td>\n",
       "      <td>0.65</td>\n",
       "      <td>12.89</td>\n",
       "      <td>3.06</td>\n",
       "      <td>51.83</td>\n",
       "      <td>...</td>\n",
       "      <td>457</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25977</th>\n",
       "      <td>28.84</td>\n",
       "      <td>10.73</td>\n",
       "      <td>28.79</td>\n",
       "      <td>326.69</td>\n",
       "      <td>408.11</td>\n",
       "      <td>1076.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>740</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21366</th>\n",
       "      <td>31.11</td>\n",
       "      <td>27.14</td>\n",
       "      <td>6.19</td>\n",
       "      <td>202.93</td>\n",
       "      <td>433.91</td>\n",
       "      <td>69.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3201</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17171</th>\n",
       "      <td>25.63</td>\n",
       "      <td>104.83</td>\n",
       "      <td>71.11</td>\n",
       "      <td>236.74</td>\n",
       "      <td>249.63</td>\n",
       "      <td>597.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>661</td>\n",
       "      <td>0.00</td>\n",
       "      <td>870.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25531</th>\n",
       "      <td>14.71</td>\n",
       "      <td>75.88</td>\n",
       "      <td>26.46</td>\n",
       "      <td>332.56</td>\n",
       "      <td>2576.34</td>\n",
       "      <td>2281.66</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>...</td>\n",
       "      <td>299</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24255</th>\n",
       "      <td>0.08</td>\n",
       "      <td>5.03</td>\n",
       "      <td>18.41</td>\n",
       "      <td>142.63</td>\n",
       "      <td>132.33</td>\n",
       "      <td>43.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2121</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14775</th>\n",
       "      <td>301.43</td>\n",
       "      <td>238.58</td>\n",
       "      <td>149.89</td>\n",
       "      <td>391.16</td>\n",
       "      <td>368.54</td>\n",
       "      <td>263.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1522</td>\n",
       "      <td>198.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>198.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>1118.64</td>\n",
       "      <td>969.99</td>\n",
       "      <td>1121.46</td>\n",
       "      <td>170.51</td>\n",
       "      <td>225.69</td>\n",
       "      <td>113.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>619</td>\n",
       "      <td>0.00</td>\n",
       "      <td>201.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18139</th>\n",
       "      <td>113.49</td>\n",
       "      <td>63.04</td>\n",
       "      <td>31.14</td>\n",
       "      <td>267.54</td>\n",
       "      <td>331.23</td>\n",
       "      <td>78.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>492</td>\n",
       "      <td>56.00</td>\n",
       "      <td>648.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25953</th>\n",
       "      <td>83.96</td>\n",
       "      <td>142.69</td>\n",
       "      <td>110.36</td>\n",
       "      <td>649.76</td>\n",
       "      <td>665.38</td>\n",
       "      <td>709.71</td>\n",
       "      <td>164.44</td>\n",
       "      <td>207.19</td>\n",
       "      <td>18.33</td>\n",
       "      <td>385.79</td>\n",
       "      <td>...</td>\n",
       "      <td>1734</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>362.21</td>\n",
       "      <td>214.58</td>\n",
       "      <td>151.98</td>\n",
       "      <td>232.46</td>\n",
       "      <td>519.03</td>\n",
       "      <td>2157.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1025</td>\n",
       "      <td>154.00</td>\n",
       "      <td>1680.00</td>\n",
       "      <td>4032.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19769</th>\n",
       "      <td>99.43</td>\n",
       "      <td>179.54</td>\n",
       "      <td>136.01</td>\n",
       "      <td>747.78</td>\n",
       "      <td>640.99</td>\n",
       "      <td>439.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1827</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20939</th>\n",
       "      <td>133.53</td>\n",
       "      <td>132.08</td>\n",
       "      <td>124.96</td>\n",
       "      <td>575.39</td>\n",
       "      <td>719.83</td>\n",
       "      <td>513.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>451</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17568</th>\n",
       "      <td>179.29</td>\n",
       "      <td>233.06</td>\n",
       "      <td>165.16</td>\n",
       "      <td>7.58</td>\n",
       "      <td>21.09</td>\n",
       "      <td>29.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>351</td>\n",
       "      <td>388.00</td>\n",
       "      <td>490.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.81</td>\n",
       "      <td>33.83</td>\n",
       "      <td>44.08</td>\n",
       "      <td>68.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>420</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>834.00</td>\n",
       "      <td>1728.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>13.64</td>\n",
       "      <td>21.44</td>\n",
       "      <td>31.11</td>\n",
       "      <td>46.48</td>\n",
       "      <td>86.24</td>\n",
       "      <td>78.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>277</td>\n",
       "      <td>198.00</td>\n",
       "      <td>2432.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>43.96</td>\n",
       "      <td>55.64</td>\n",
       "      <td>51.86</td>\n",
       "      <td>219.91</td>\n",
       "      <td>116.51</td>\n",
       "      <td>177.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3025</td>\n",
       "      <td>252.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>1806.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>375.44</td>\n",
       "      <td>370.13</td>\n",
       "      <td>262.31</td>\n",
       "      <td>603.68</td>\n",
       "      <td>750.44</td>\n",
       "      <td>411.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.03</td>\n",
       "      <td>274.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1589</td>\n",
       "      <td>0.00</td>\n",
       "      <td>794.00</td>\n",
       "      <td>145.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23333</th>\n",
       "      <td>884.34</td>\n",
       "      <td>1196.29</td>\n",
       "      <td>945.49</td>\n",
       "      <td>652.13</td>\n",
       "      <td>645.03</td>\n",
       "      <td>673.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1316</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26967</th>\n",
       "      <td>602.06</td>\n",
       "      <td>1487.38</td>\n",
       "      <td>1075.39</td>\n",
       "      <td>376.83</td>\n",
       "      <td>408.49</td>\n",
       "      <td>173.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>738</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>4.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.13</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.83</td>\n",
       "      <td>31.61</td>\n",
       "      <td>7.38</td>\n",
       "      <td>3.81</td>\n",
       "      <td>...</td>\n",
       "      <td>1537</td>\n",
       "      <td>2432.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>0.00</td>\n",
       "      <td>7.20</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.85</td>\n",
       "      <td>16.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>812</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3755.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2697</td>\n",
       "      <td>154.00</td>\n",
       "      <td>1008.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16023</th>\n",
       "      <td>10.83</td>\n",
       "      <td>1016.34</td>\n",
       "      <td>1507.28</td>\n",
       "      <td>16.96</td>\n",
       "      <td>855.14</td>\n",
       "      <td>827.94</td>\n",
       "      <td>6.11</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.79</td>\n",
       "      <td>...</td>\n",
       "      <td>1075</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27495</th>\n",
       "      <td>52.84</td>\n",
       "      <td>46.24</td>\n",
       "      <td>24.83</td>\n",
       "      <td>229.04</td>\n",
       "      <td>138.54</td>\n",
       "      <td>309.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>230</td>\n",
       "      <td>274.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11363</th>\n",
       "      <td>86.53</td>\n",
       "      <td>127.06</td>\n",
       "      <td>172.79</td>\n",
       "      <td>170.28</td>\n",
       "      <td>229.31</td>\n",
       "      <td>244.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>198.00</td>\n",
       "      <td>198.00</td>\n",
       "      <td>575.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28020</th>\n",
       "      <td>85.29</td>\n",
       "      <td>92.14</td>\n",
       "      <td>150.24</td>\n",
       "      <td>256.29</td>\n",
       "      <td>237.41</td>\n",
       "      <td>261.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2450</td>\n",
       "      <td>686.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14423</th>\n",
       "      <td>224.94</td>\n",
       "      <td>211.11</td>\n",
       "      <td>51.43</td>\n",
       "      <td>268.88</td>\n",
       "      <td>169.09</td>\n",
       "      <td>54.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2471</td>\n",
       "      <td>669.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>608.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>1467.26</td>\n",
       "      <td>1833.58</td>\n",
       "      <td>2165.33</td>\n",
       "      <td>1110.24</td>\n",
       "      <td>169.83</td>\n",
       "      <td>161.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>492</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>112.79</td>\n",
       "      <td>172.56</td>\n",
       "      <td>425.14</td>\n",
       "      <td>416.94</td>\n",
       "      <td>508.81</td>\n",
       "      <td>513.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2921</td>\n",
       "      <td>252.00</td>\n",
       "      <td>252.00</td>\n",
       "      <td>1414.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16850</th>\n",
       "      <td>4.44</td>\n",
       "      <td>0.70</td>\n",
       "      <td>32.89</td>\n",
       "      <td>19.46</td>\n",
       "      <td>23.21</td>\n",
       "      <td>41.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2679</td>\n",
       "      <td>0.00</td>\n",
       "      <td>616.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>10.84</td>\n",
       "      <td>74.63</td>\n",
       "      <td>293.66</td>\n",
       "      <td>53.63</td>\n",
       "      <td>32.24</td>\n",
       "      <td>379.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>671</td>\n",
       "      <td>1008.00</td>\n",
       "      <td>1008.00</td>\n",
       "      <td>1008.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22118</th>\n",
       "      <td>113.88</td>\n",
       "      <td>108.89</td>\n",
       "      <td>151.03</td>\n",
       "      <td>357.89</td>\n",
       "      <td>287.43</td>\n",
       "      <td>361.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3210</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>517.94</td>\n",
       "      <td>527.78</td>\n",
       "      <td>564.51</td>\n",
       "      <td>293.53</td>\n",
       "      <td>395.23</td>\n",
       "      <td>291.49</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.45</td>\n",
       "      <td>3.53</td>\n",
       "      <td>...</td>\n",
       "      <td>715</td>\n",
       "      <td>225.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>25.71</td>\n",
       "      <td>49.73</td>\n",
       "      <td>60.41</td>\n",
       "      <td>181.06</td>\n",
       "      <td>136.79</td>\n",
       "      <td>272.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>958</td>\n",
       "      <td>1197.00</td>\n",
       "      <td>795.00</td>\n",
       "      <td>716.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>48.13</td>\n",
       "      <td>22.29</td>\n",
       "      <td>68.29</td>\n",
       "      <td>1050.54</td>\n",
       "      <td>878.53</td>\n",
       "      <td>1605.18</td>\n",
       "      <td>8.26</td>\n",
       "      <td>34.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.01</td>\n",
       "      <td>...</td>\n",
       "      <td>921</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>74.04</td>\n",
       "      <td>54.11</td>\n",
       "      <td>143.09</td>\n",
       "      <td>44.21</td>\n",
       "      <td>44.46</td>\n",
       "      <td>176.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2934</td>\n",
       "      <td>198.00</td>\n",
       "      <td>198.00</td>\n",
       "      <td>198.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>2.98</td>\n",
       "      <td>401.01</td>\n",
       "      <td>99.49</td>\n",
       "      <td>40.83</td>\n",
       "      <td>1496.54</td>\n",
       "      <td>1369.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>350</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>247.36</td>\n",
       "      <td>27.29</td>\n",
       "      <td>150.51</td>\n",
       "      <td>229.53</td>\n",
       "      <td>222.09</td>\n",
       "      <td>301.43</td>\n",
       "      <td>218.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>364.91</td>\n",
       "      <td>...</td>\n",
       "      <td>3651</td>\n",
       "      <td>92.00</td>\n",
       "      <td>575.00</td>\n",
       "      <td>575.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>19.58</td>\n",
       "      <td>31.68</td>\n",
       "      <td>29.88</td>\n",
       "      <td>121.23</td>\n",
       "      <td>70.38</td>\n",
       "      <td>186.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2369</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19952 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       onnet_mou_6  onnet_mou_7  onnet_mou_8  offnet_mou_6  offnet_mou_7  \\\n",
       "11465       125.09        81.18        58.99         11.28          6.44   \n",
       "21150        19.33        85.69        55.01       1002.11       1301.99   \n",
       "2498          4.21         3.68         0.00         15.66          7.84   \n",
       "13702        58.33        82.51        50.18        354.54        351.79   \n",
       "20184        65.14        76.81        45.06        483.84        546.34   \n",
       "5502         80.64       272.33       462.04        259.03        372.08   \n",
       "10325         0.80         3.58         2.96          8.84          3.86   \n",
       "5995        148.86         7.86         3.19         13.28         37.01   \n",
       "6228        426.48       442.51       482.33        862.38       1072.79   \n",
       "23573        59.68        88.46        75.69        458.89        407.53   \n",
       "10690         0.83         0.00         0.00          8.51          0.00   \n",
       "1199        186.13        81.58        42.16        599.24        255.68   \n",
       "26114       304.36       193.26       311.19        570.19        284.49   \n",
       "1692        137.36       152.41       154.33        719.11        650.31   \n",
       "19652       749.33       584.66       975.39        996.61        789.48   \n",
       "1709          9.14        14.81        30.46         63.63        182.79   \n",
       "11500         4.19         0.66         6.11        129.76        102.09   \n",
       "9621          0.26         0.56         0.00          8.74          4.14   \n",
       "28053        24.33         0.00         0.00        174.76          2.34   \n",
       "25995       248.41       388.91       364.36        728.19        571.14   \n",
       "22380       161.39      1126.93       805.14         41.68        114.79   \n",
       "25977        28.84        10.73        28.79        326.69        408.11   \n",
       "21366        31.11        27.14         6.19        202.93        433.91   \n",
       "17171        25.63       104.83        71.11        236.74        249.63   \n",
       "25531        14.71        75.88        26.46        332.56       2576.34   \n",
       "24255         0.08         5.03        18.41        142.63        132.33   \n",
       "14775       301.43       238.58       149.89        391.16        368.54   \n",
       "1608       1118.64       969.99      1121.46        170.51        225.69   \n",
       "18139       113.49        63.04        31.14        267.54        331.23   \n",
       "25953        83.96       142.69       110.36        649.76        665.38   \n",
       "...            ...          ...          ...           ...           ...   \n",
       "6396        362.21       214.58       151.98        232.46        519.03   \n",
       "19769        99.43       179.54       136.01        747.78        640.99   \n",
       "20939       133.53       132.08       124.96        575.39        719.83   \n",
       "17568       179.29       233.06       165.16          7.58         21.09   \n",
       "6420          0.93         0.10         5.81         33.83         44.08   \n",
       "5051         13.64        21.44        31.11         46.48         86.24   \n",
       "5311         43.96        55.64        51.86        219.91        116.51   \n",
       "2433        375.44       370.13       262.31        603.68        750.44   \n",
       "23333       884.34      1196.29       945.49        652.13        645.03   \n",
       "26967       602.06      1487.38      1075.39        376.83        408.49   \n",
       "769           4.88         0.00         0.00          4.13          1.00   \n",
       "1685          0.00         7.20         3.89          0.00          2.85   \n",
       "8322          0.00         0.00         0.85          0.00          0.61   \n",
       "16023        10.83      1016.34      1507.28         16.96        855.14   \n",
       "27495        52.84        46.24        24.83        229.04        138.54   \n",
       "11363        86.53       127.06       172.79        170.28        229.31   \n",
       "28020        85.29        92.14       150.24        256.29        237.41   \n",
       "14423       224.94       211.11        51.43        268.88        169.09   \n",
       "21962      1467.26      1833.58      2165.33       1110.24        169.83   \n",
       "4426        112.79       172.56       425.14        416.94        508.81   \n",
       "16850         4.44         0.70        32.89         19.46         23.21   \n",
       "6265         10.84        74.63       293.66         53.63         32.24   \n",
       "22118       113.88       108.89       151.03        357.89        287.43   \n",
       "11284       517.94       527.78       564.51        293.53        395.23   \n",
       "11964        25.71        49.73        60.41        181.06        136.79   \n",
       "21575        48.13        22.29        68.29       1050.54        878.53   \n",
       "5390         74.04        54.11       143.09         44.21         44.46   \n",
       "860           2.98       401.01        99.49         40.83       1496.54   \n",
       "15795       247.36        27.29       150.51        229.53        222.09   \n",
       "23654        19.58        31.68        29.88        121.23         70.38   \n",
       "\n",
       "       offnet_mou_8  roam_ic_mou_6  roam_ic_mou_7  roam_ic_mou_8  \\\n",
       "11465         16.88           0.00           0.00           0.00   \n",
       "21150       1378.78         387.41           0.00           0.00   \n",
       "2498          11.86           0.00           0.00           0.00   \n",
       "13702        252.21           3.03           0.00           0.00   \n",
       "20184        258.24           0.66           9.05           0.00   \n",
       "5502         124.06           0.00           0.00           0.00   \n",
       "10325          1.83           0.00           0.00           0.00   \n",
       "5995           8.03           0.00           0.00           0.00   \n",
       "6228         868.76           0.00           0.00           0.00   \n",
       "23573        231.89           0.00           0.00           0.00   \n",
       "10690          2.20           0.00           0.00           0.00   \n",
       "1199          49.94          20.23           0.00           0.00   \n",
       "26114        218.81           0.00           0.00           0.00   \n",
       "1692         505.48           0.00          23.36           7.44   \n",
       "19652        422.71           0.00           0.00           0.00   \n",
       "1709         254.54           0.00           0.00           0.00   \n",
       "11500         49.13           0.00           0.00           0.00   \n",
       "9621           0.00           0.00           0.00           0.00   \n",
       "28053          0.00           0.00           0.00           0.00   \n",
       "25995        930.46           0.00           0.00           0.00   \n",
       "22380         52.09           0.65          12.89           3.06   \n",
       "25977       1076.31           0.00           0.00           0.00   \n",
       "21366         69.73           0.00           0.00           0.00   \n",
       "17171        597.18           0.00           0.00           0.00   \n",
       "25531       2281.66           0.28           0.00           2.33   \n",
       "24255         43.13           0.00           0.00           0.00   \n",
       "14775        263.74           0.00           0.00           0.00   \n",
       "1608         113.98           0.00           0.00           0.00   \n",
       "18139         78.53           0.00           0.00          10.79   \n",
       "25953        709.71         164.44         207.19          18.33   \n",
       "...             ...            ...            ...            ...   \n",
       "6396        2157.43           0.00           0.00          25.56   \n",
       "19769        439.76           0.00           0.00           0.00   \n",
       "20939        513.63           0.00           0.00           0.00   \n",
       "17568         29.31           0.00           0.00           7.26   \n",
       "6420          68.88           0.00           0.00           0.00   \n",
       "5051          78.54           0.00           0.00           0.00   \n",
       "5311         177.13           0.00           0.00           0.00   \n",
       "2433         411.94           0.00          25.03         274.09   \n",
       "23333        673.41           0.00           0.00           5.63   \n",
       "26967        173.33           0.00           0.00           0.00   \n",
       "769            0.00          33.83          31.61           7.38   \n",
       "1685          16.31           0.00           0.00           0.00   \n",
       "8322           0.70           0.00           0.00           0.00   \n",
       "16023        827.94           6.11           0.89           0.00   \n",
       "27495        309.24           0.00           0.00           0.00   \n",
       "11363        244.73           0.00           0.00           0.00   \n",
       "28020        261.11           0.00           0.00           0.00   \n",
       "14423         54.16           0.00           0.00          20.83   \n",
       "21962        161.99           0.00           0.00           0.00   \n",
       "4426         513.61           0.00           0.00           0.00   \n",
       "16850         41.93           0.00           0.00           0.00   \n",
       "6265         379.64           0.00           0.00           0.00   \n",
       "22118        361.84           0.00           0.00           0.00   \n",
       "11284        291.49           4.45           0.00           1.45   \n",
       "11964        272.24           0.00           0.00           0.00   \n",
       "21575       1605.18           8.26          34.03           0.00   \n",
       "5390         176.09           0.00           0.00           0.00   \n",
       "860         1369.79           0.00           0.00           8.31   \n",
       "15795        301.43         218.33           0.00           0.00   \n",
       "23654        186.36           0.00           0.00           0.00   \n",
       "\n",
       "       roam_og_mou_6          ...             aon  total_rech_data_amt_6  \\\n",
       "11465           0.00          ...            2845                 580.00   \n",
       "21150         105.79          ...            2867                   0.00   \n",
       "2498            0.00          ...             384                   0.00   \n",
       "13702           6.85          ...            1122                 368.00   \n",
       "20184           2.60          ...            3098                   0.00   \n",
       "5502            0.00          ...             572                 225.00   \n",
       "10325           0.00          ...             793                 252.00   \n",
       "5995            0.00          ...             278                 666.00   \n",
       "6228            0.00          ...            2576                1008.00   \n",
       "23573           0.00          ...            3651                   0.00   \n",
       "10690           0.00          ...             225                1640.00   \n",
       "1199           19.83          ...             858                 248.00   \n",
       "26114           0.00          ...            2234                   0.00   \n",
       "1692            0.00          ...             363                   0.00   \n",
       "19652           0.00          ...            1104                   0.00   \n",
       "1709            0.00          ...             240                   0.00   \n",
       "11500           0.00          ...            2983                 608.00   \n",
       "9621            0.00          ...             474                 879.00   \n",
       "28053           0.00          ...             251                1372.00   \n",
       "25995           0.00          ...             422                   0.00   \n",
       "22380          51.83          ...             457                   0.00   \n",
       "25977           0.00          ...             740                   0.00   \n",
       "21366           0.00          ...            3201                   0.00   \n",
       "17171           0.00          ...             661                   0.00   \n",
       "25531           6.51          ...             299                   0.00   \n",
       "24255           0.00          ...            2121                   0.00   \n",
       "14775           0.00          ...            1522                 198.00   \n",
       "1608            0.00          ...             619                   0.00   \n",
       "18139           0.00          ...             492                  56.00   \n",
       "25953         385.79          ...            1734                   0.00   \n",
       "...              ...          ...             ...                    ...   \n",
       "6396            0.00          ...            1025                 154.00   \n",
       "19769           0.00          ...            1827                   0.00   \n",
       "20939           0.00          ...             451                   0.00   \n",
       "17568           0.00          ...             351                 388.00   \n",
       "6420            0.00          ...             420                1950.00   \n",
       "5051            0.00          ...             277                 198.00   \n",
       "5311            0.00          ...            3025                 252.00   \n",
       "2433            0.00          ...            1589                   0.00   \n",
       "23333           0.00          ...            1316                   0.00   \n",
       "26967           0.00          ...             738                   0.00   \n",
       "769             3.81          ...            1537                2432.00   \n",
       "1685            0.00          ...             812                   0.00   \n",
       "8322            0.00          ...            2697                 154.00   \n",
       "16023          27.79          ...            1075                   0.00   \n",
       "27495           0.00          ...             230                 274.00   \n",
       "11363           0.00          ...             224                 198.00   \n",
       "28020           0.00          ...            2450                 686.00   \n",
       "14423           0.00          ...            2471                 669.00   \n",
       "21962           0.00          ...             492                   0.00   \n",
       "4426            0.00          ...            2921                 252.00   \n",
       "16850           0.00          ...            2679                   0.00   \n",
       "6265            0.00          ...             671                1008.00   \n",
       "22118           0.00          ...            3210                   0.00   \n",
       "11284           3.53          ...             715                 225.00   \n",
       "11964           0.00          ...             958                1197.00   \n",
       "21575          39.01          ...             921                   0.00   \n",
       "5390            0.00          ...            2934                 198.00   \n",
       "860             0.00          ...             350                   8.00   \n",
       "15795         364.91          ...            3651                  92.00   \n",
       "23654           0.00          ...            2369                   0.00   \n",
       "\n",
       "       total_rech_data_amt_7  total_rech_data_amt_8  rech_days_left_6  \\\n",
       "11465                 252.00                 145.00              2.00   \n",
       "21150                   0.00                   0.00              0.00   \n",
       "2498                 2496.00                 856.00             13.00   \n",
       "13702                 368.00                 154.00              1.00   \n",
       "20184                   0.00                   0.00              3.00   \n",
       "5502                 1225.00                1225.00              0.00   \n",
       "10325                 252.00                 252.00             13.00   \n",
       "5995                 1036.00                 768.00             11.00   \n",
       "6228                  252.00                 252.00              2.00   \n",
       "23573                   0.00                   0.00              5.00   \n",
       "10690                1016.00                1215.00              1.00   \n",
       "1199                   17.00                   0.00              0.00   \n",
       "26114                   0.00                   0.00              5.00   \n",
       "1692                  252.00                 616.00              1.00   \n",
       "19652                   0.00                   0.00              0.00   \n",
       "1709                 2088.00                2180.00             17.00   \n",
       "11500                 608.00                2432.00              9.00   \n",
       "9621                   98.00                 505.00              8.00   \n",
       "28053                   0.00                   0.00             22.00   \n",
       "25995                   0.00                   0.00              0.00   \n",
       "22380                   0.00                   0.00              2.00   \n",
       "25977                   0.00                   0.00              3.00   \n",
       "21366                   0.00                   0.00              0.00   \n",
       "17171                 870.00                   0.00              0.00   \n",
       "25531                   0.00                   0.00              0.00   \n",
       "24255                   0.00                   0.00              8.00   \n",
       "14775                   0.00                 198.00              0.00   \n",
       "1608                  201.00                  25.00              0.00   \n",
       "18139                 648.00                   0.00              1.00   \n",
       "25953                   0.00                   0.00              4.00   \n",
       "...                      ...                    ...               ...   \n",
       "6396                 1680.00                4032.00              4.00   \n",
       "19769                   0.00                   0.00              3.00   \n",
       "20939                   0.00                   0.00              1.00   \n",
       "17568                 490.00                   0.00              0.00   \n",
       "6420                  834.00                1728.00              0.00   \n",
       "5051                 2432.00                 252.00              2.00   \n",
       "5311                  252.00                1806.00             11.00   \n",
       "2433                  794.00                 145.00              2.00   \n",
       "23333                   0.00                   0.00              0.00   \n",
       "26967                   0.00                   0.00              0.00   \n",
       "769                     0.00                   0.00             16.00   \n",
       "1685                 3755.00                  17.00             17.00   \n",
       "8322                 1008.00                 252.00             17.00   \n",
       "16023                   0.00                  23.00              0.00   \n",
       "27495                   0.00                   0.00              0.00   \n",
       "11363                 198.00                 575.00              8.00   \n",
       "28020                   0.00                   0.00              6.00   \n",
       "14423                   0.00                 608.00              3.00   \n",
       "21962                   0.00                   0.00              0.00   \n",
       "4426                  252.00                1414.00              4.00   \n",
       "16850                 616.00                   0.00              0.00   \n",
       "6265                 1008.00                1008.00              7.00   \n",
       "22118                   0.00                   0.00              0.00   \n",
       "11284                 100.00                 100.00              2.00   \n",
       "11964                 795.00                 716.00              1.00   \n",
       "21575                   0.00                   0.00             14.00   \n",
       "5390                  198.00                 198.00              9.00   \n",
       "860                     0.00                   0.00              6.00   \n",
       "15795                 575.00                 575.00              1.00   \n",
       "23654                   0.00                   0.00              1.00   \n",
       "\n",
       "       rech_days_left_data_6  rech_days_left_7  rech_days_left_data_7  \\\n",
       "11465                   2.00              6.00                   6.00   \n",
       "21150                   0.00              0.00                   0.00   \n",
       "2498                    0.00              2.00                   2.00   \n",
       "13702                   1.00              4.00                   6.00   \n",
       "20184                   0.00              6.00                   0.00   \n",
       "5502                    2.00              2.00                   5.00   \n",
       "10325                  13.00              7.00                  17.00   \n",
       "5995                   11.00             11.00                  16.00   \n",
       "6228                    2.00              0.00                  12.00   \n",
       "23573                   0.00             13.00                   0.00   \n",
       "10690                   1.00              2.00                   2.00   \n",
       "1199                    0.00              1.00                  10.00   \n",
       "26114                   0.00              8.00                   0.00   \n",
       "1692                    0.00              4.00                  20.00   \n",
       "19652                   0.00              0.00                   0.00   \n",
       "1709                    0.00              0.00                   0.00   \n",
       "11500                  25.00             13.00                  27.00   \n",
       "9621                    8.00             11.00                  26.00   \n",
       "28053                  22.00              8.00                   0.00   \n",
       "25995                   0.00              5.00                   0.00   \n",
       "22380                   0.00              4.00                   0.00   \n",
       "25977                   0.00              1.00                   0.00   \n",
       "21366                   0.00              1.00                   0.00   \n",
       "17171                   0.00              0.00                   2.00   \n",
       "25531                   0.00              0.00                   0.00   \n",
       "24255                   0.00              8.00                   0.00   \n",
       "14775                  21.00              3.00                   0.00   \n",
       "1608                    0.00              0.00                  26.00   \n",
       "18139                   2.00              5.00                  10.00   \n",
       "25953                   0.00              5.00                   0.00   \n",
       "...                      ...               ...                    ...   \n",
       "6396                   17.00              1.00                   1.00   \n",
       "19769                   0.00              0.00                   0.00   \n",
       "20939                   0.00              3.00                   0.00   \n",
       "17568                   4.00              5.00                  14.00   \n",
       "6420                    2.00              5.00                   5.00   \n",
       "5051                   25.00              2.00                   2.00   \n",
       "5311                   21.00              0.00                  24.00   \n",
       "2433                    0.00              0.00                   3.00   \n",
       "23333                   0.00              2.00                   0.00   \n",
       "26967                   0.00              1.00                   0.00   \n",
       "769                    16.00              3.00                   0.00   \n",
       "1685                    0.00              1.00                   1.00   \n",
       "8322                   26.00              1.00                   1.00   \n",
       "16023                   0.00              1.00                   0.00   \n",
       "27495                  20.00              6.00                   0.00   \n",
       "11363                   8.00              0.00                   5.00   \n",
       "28020                  11.00             15.00                   0.00   \n",
       "14423                   3.00              5.00                   0.00   \n",
       "21962                   0.00              0.00                   0.00   \n",
       "4426                   20.00              7.00                  23.00   \n",
       "16850                   0.00              6.00                   6.00   \n",
       "6265                   10.00              7.00                  19.00   \n",
       "22118                   0.00              0.00                   0.00   \n",
       "11284                   3.00              2.00                   6.00   \n",
       "11964                   1.00              1.00                   5.00   \n",
       "21575                   0.00              0.00                   0.00   \n",
       "5390                    9.00             12.00                  12.00   \n",
       "860                    29.00              0.00                   0.00   \n",
       "15795                   1.00              0.00                   0.00   \n",
       "23654                   0.00              0.00                   0.00   \n",
       "\n",
       "       rech_days_left_8  rech_days_left_data_8  \n",
       "11465              9.00                   9.00  \n",
       "21150              1.00                   0.00  \n",
       "2498               7.00                  14.00  \n",
       "13702              0.00                  17.00  \n",
       "20184             10.00                   0.00  \n",
       "5502               4.00                   4.00  \n",
       "10325             21.00                  21.00  \n",
       "5995              13.00                  13.00  \n",
       "6228               7.00                  21.00  \n",
       "23573              3.00                   0.00  \n",
       "10690              0.00                   0.00  \n",
       "1199              27.00                   0.00  \n",
       "26114             10.00                   0.00  \n",
       "1692               6.00                   6.00  \n",
       "19652              2.00                   0.00  \n",
       "1709               2.00                   8.00  \n",
       "11500              1.00                   1.00  \n",
       "9621               1.00                   1.00  \n",
       "28053              0.00                   0.00  \n",
       "25995              3.00                   0.00  \n",
       "22380              0.00                   0.00  \n",
       "25977              2.00                   0.00  \n",
       "21366              5.00                   0.00  \n",
       "17171              1.00                   0.00  \n",
       "25531              1.00                   0.00  \n",
       "24255              7.00                   0.00  \n",
       "14775              6.00                  28.00  \n",
       "1608               0.00                   2.00  \n",
       "18139              3.00                   0.00  \n",
       "25953              2.00                   0.00  \n",
       "...                 ...                    ...  \n",
       "6396               0.00                   0.00  \n",
       "19769              4.00                   0.00  \n",
       "20939              7.00                   0.00  \n",
       "17568              6.00                   0.00  \n",
       "6420               1.00                   1.00  \n",
       "5051               3.00                   7.00  \n",
       "5311               2.00                   2.00  \n",
       "2433               1.00                   1.00  \n",
       "23333              0.00                   0.00  \n",
       "26967              1.00                   0.00  \n",
       "769                5.00                   0.00  \n",
       "1685              15.00                  23.00  \n",
       "8322              10.00                  10.00  \n",
       "16023              4.00                  20.00  \n",
       "27495              1.00                   0.00  \n",
       "11363              0.00                   0.00  \n",
       "28020              4.00                   0.00  \n",
       "14423              7.00                  29.00  \n",
       "21962              2.00                   0.00  \n",
       "4426               1.00                   1.00  \n",
       "16850              6.00                   0.00  \n",
       "6265               3.00                  10.00  \n",
       "22118              1.00                   0.00  \n",
       "11284              2.00                  16.00  \n",
       "11964              6.00                  15.00  \n",
       "21575              2.00                   0.00  \n",
       "5390              15.00                  15.00  \n",
       "860                5.00                   0.00  \n",
       "15795              1.00                   1.00  \n",
       "23654              0.00                   0.00  \n",
       "\n",
       "[19952 rows x 136 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "x_scaled = scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "X_train=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30672382, -0.4074391 , -0.40038142, ...,  0.0535099 ,\n",
       "         0.97497267,  0.39784624],\n",
       "       [-0.54353344, -0.39793704, -0.40891035, ..., -0.69702047,\n",
       "        -0.60936164, -0.69354754],\n",
       "       [-0.57738897, -0.57072291, -0.52679384, ..., -0.44684368,\n",
       "         0.57888909,  1.00417612],\n",
       "       ...,\n",
       "       [-0.58014309,  0.26640684, -0.31359208, ..., -0.69702047,\n",
       "         0.18280551, -0.69354754],\n",
       "       [-0.03294629, -0.52097929, -0.20425895, ..., -0.69702047,\n",
       "        -0.60936164, -0.57228156],\n",
       "       [-0.54297366, -0.51173005, -0.46276259, ..., -0.69702047,\n",
       "        -0.80740343, -0.69354754]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48854058, -0.51324701, -0.46473411, ...,  1.05421706,\n",
       "        -0.80740343, -0.08721766],\n",
       "       [ 0.69481336,  0.22100341,  0.01342157, ..., -0.69702047,\n",
       "         0.77693088, -0.69354754],\n",
       "       [ 0.08904111,  0.2495096 ,  0.22411608, ..., -0.69702047,\n",
       "        -0.21327806, -0.69354754],\n",
       "       ...,\n",
       "       [ 1.37517079,  2.0773823 ,  1.85142686, ..., -0.69702047,\n",
       "         1.17301446, -0.69354754],\n",
       "       [-0.31216489, -0.52978608, -0.42733969, ..., -0.69702047,\n",
       "        -0.60936164, -0.69354754],\n",
       "       [ 1.70669529, -0.42764415,  0.44070369, ..., -0.69702047,\n",
       "        -0.80740343, -0.69354754]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying SVM kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(C = 1, kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# confusion matrix\n",
    "confusion_matrix(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      8074\n",
      "          1       0.00      0.00      0.00       478\n",
      "\n",
      "avg / total       0.89      0.94      0.92      8552\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkumar5\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Printing classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing decision tree classifier from sklearn library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Fitting the decision tree with default hyperparameters, apart from\n",
    "# max_depth which is 5 so that we can plot and read the tree.\n",
    "dt_default = DecisionTreeClassifier(max_depth=5)\n",
    "dt_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98      8092\n",
      "          1       0.62      0.33      0.43       460\n",
      "\n",
      "avg / total       0.94      0.95      0.95      8552\n",
      "\n",
      "[0 0 0 ... 0 0 0]\n",
      "11701    0\n",
      "26714    0\n",
      "20293    0\n",
      "1433     1\n",
      "25489    0\n",
      "17038    0\n",
      "15091    0\n",
      "11021    0\n",
      "4178     0\n",
      "15565    0\n",
      "28393    0\n",
      "27385    0\n",
      "15622    0\n",
      "4084     0\n",
      "5658     0\n",
      "19598    0\n",
      "26140    0\n",
      "866      1\n",
      "17769    0\n",
      "23005    0\n",
      "24108    0\n",
      "8005     0\n",
      "18393    0\n",
      "3426     0\n",
      "26436    0\n",
      "4416     0\n",
      "21183    0\n",
      "2129     0\n",
      "25831    0\n",
      "22616    0\n",
      "        ..\n",
      "18197    0\n",
      "7658     0\n",
      "27080    0\n",
      "8018     0\n",
      "7290     0\n",
      "23971    0\n",
      "1348     1\n",
      "3264     0\n",
      "17935    0\n",
      "24877    0\n",
      "6161     0\n",
      "28409    0\n",
      "3388     0\n",
      "3310     0\n",
      "25617    0\n",
      "21314    0\n",
      "18475    0\n",
      "14655    0\n",
      "18429    0\n",
      "6883     0\n",
      "23716    0\n",
      "12848    0\n",
      "24012    0\n",
      "5709     0\n",
      "23638    0\n",
      "22529    0\n",
      "20553    0\n",
      "21418    0\n",
      "24770    0\n",
      "28157    0\n",
      "Name: churn, Length: 8552, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's check the evaluation metrics of our default model\n",
    "\n",
    "# Importing classification report and confusion matrix from sklearn metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Making predictions\n",
    "y_pred_default = dt_default.predict(X_test)\n",
    "\n",
    "# Printing classification report\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "\n",
    "print(y_pred_default)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction using lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19952, 73)\n",
      "[  0   1   5   6   7   8   9  10  11  15  20  21  23  30  31  34  36  38\n",
      "  39  42  43  44  46  47  49  50  54  55  58  61  62  63  65  71  75  76\n",
      "  77  78  80  81  83  87  88  89  90  91  92  93  94  95  97  98  99 100\n",
      " 101 102 103 104 106 107 108 111 112 113 114 120 121 125 126 130 132 134\n",
      " 135]\n",
      "(8552, 73)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    " \n",
    "lsvc = LinearSVC(C=0.02, penalty=\"l1\", dual=False).fit(X_train,y_train)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_train_transformed = model.transform(X_train)\n",
    "pos = model.get_support(indices=True)\n",
    " \n",
    "print(X_train_transformed.shape)\n",
    "print(pos)\n",
    " \n",
    "X_test_transformed = model.transform(X_test)\n",
    "print(X_test_transformed.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37632, 73)\n",
      "(37632,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18816"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X = X_train_transformed  #telecom.drop(['churn'],axis=1)\n",
    "y = y_train  #telecom['churn']\n",
    "sm = SMOTE(kind = \"regular\")\n",
    "X_tr,y_tr = sm.fit_sample(X,y)\n",
    "print(X_tr.shape)\n",
    "print(y_tr.shape)\n",
    "np.count_nonzero(y_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying linear SVC after lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = SVC(C = 1)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "confusion_matrix(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# applying default random forest after lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier from sklearn library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "#rfc.fit(X_train_transformed,y_train)\n",
    "rfc.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = rfc.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      8092\n",
      "          1       0.44      0.47      0.45       460\n",
      "\n",
      "avg / total       0.94      0.94      0.94      8552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the report of our default model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7819  273]\n",
      " [ 246  214]]\n"
     ]
    }
   ],
   "source": [
    "# Printing confusion matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393124415341441\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': range(2, 20, 5)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal n_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(2, 20, 5)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 7}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>12</td>\n",
       "      <td>{'max_depth': 12}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17</td>\n",
       "      <td>{'max_depth': 17}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0           0.48             0.02             1.00              1.00   \n",
       "1           0.67             0.02             1.00              1.00   \n",
       "2           0.63             0.02             1.00              1.00   \n",
       "3           0.67             0.02             1.00              1.00   \n",
       "\n",
       "  param_max_depth             params  rank_test_score  split0_test_score  \\\n",
       "0               2   {'max_depth': 2}                4               1.00   \n",
       "1               7   {'max_depth': 7}                1               1.00   \n",
       "2              12  {'max_depth': 12}                1               1.00   \n",
       "3              17  {'max_depth': 17}                1               1.00   \n",
       "\n",
       "   split0_train_score  split1_test_score       ...         split2_test_score  \\\n",
       "0                1.00               1.00       ...                      1.00   \n",
       "1                1.00               1.00       ...                      1.00   \n",
       "2                1.00               1.00       ...                      1.00   \n",
       "3                1.00               1.00       ...                      1.00   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0                1.00               1.00                1.00   \n",
       "1                1.00               1.00                1.00   \n",
       "2                1.00               1.00                1.00   \n",
       "3                1.00               1.00                1.00   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0               1.00                1.00          0.05            0.00   \n",
       "1               1.00                1.00          0.07            0.01   \n",
       "2               1.00                1.00          0.10            0.00   \n",
       "3               1.00                1.00          0.03            0.00   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0            0.00             0.00  \n",
       "1            0.00             0.00  \n",
       "2            0.00             0.00  \n",
       "3            0.00             0.00  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFhCAYAAAD+2OlvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERsUthQUREROxSWBARERG7/hc8pBfL3FIFdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracies with max_depth\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': range(100, 1500, 400)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal n_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'n_estimators': range(100, 1500, 400)}\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "rf = RandomForestClassifier(max_depth=7)\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.62</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>500</td>\n",
       "      <td>{'n_estimators': 500}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.97</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>900</td>\n",
       "      <td>{'n_estimators': 900}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.40</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1300</td>\n",
       "      <td>{'n_estimators': 1300}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0           6.16             0.11             1.00              1.00   \n",
       "1          20.62             0.31             1.00              1.00   \n",
       "2          29.97             0.47             1.00              1.00   \n",
       "3          43.40             0.70             1.00              1.00   \n",
       "\n",
       "  param_n_estimators                  params  rank_test_score  \\\n",
       "0                100   {'n_estimators': 100}                1   \n",
       "1                500   {'n_estimators': 500}                1   \n",
       "2                900   {'n_estimators': 900}                1   \n",
       "3               1300  {'n_estimators': 1300}                1   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score       ...         \\\n",
       "0               1.00                1.00               1.00       ...          \n",
       "1               1.00                1.00               1.00       ...          \n",
       "2               1.00                1.00               1.00       ...          \n",
       "3               1.00                1.00               1.00       ...          \n",
       "\n",
       "   split2_test_score  split2_train_score  split3_test_score  \\\n",
       "0               1.00                1.00               1.00   \n",
       "1               1.00                1.00               1.00   \n",
       "2               1.00                1.00               1.00   \n",
       "3               1.00                1.00               1.00   \n",
       "\n",
       "   split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "0                1.00               1.00                1.00          0.28   \n",
       "1                1.00               1.00                1.00          5.67   \n",
       "2                1.00               1.00                1.00          0.66   \n",
       "3                1.00               1.00                1.00          0.52   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0            0.00            0.00             0.00  \n",
       "1            0.09            0.00             0.00  \n",
       "2            0.01            0.00             0.00  \n",
       "3            0.04            0.00             0.00  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting accuracies with n_estimators\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_n_estimators\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal max_features\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_features': [4, 8, 14, 20, 24]}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier(max_depth=4)\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting accuracies with max_features\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_features\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_features\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal min_samples_leaf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_leaf': range(100, 400, 50)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal min_samples_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_split': range(200, 500, 50)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = rf.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting accuracies with min_samples_split\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [4,8,10],\n",
    "    'min_samples_leaf': range(100, 400, 200),\n",
    "    'min_samples_split': range(200, 500, 200),\n",
    "    'n_estimators': [100,200, 300], \n",
    "    'max_features': [5, 10]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=10,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=200,\n",
    "                             max_features=10,\n",
    "                             n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(X_train_transformed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = rfc.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_default = DecisionTreeClassifier(max_depth=5)\n",
    "dt_default.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      8128\n",
      "          1       0.59      0.26      0.36       531\n",
      "\n",
      "avg / total       0.93      0.94      0.93      8659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_default = dt_default.predict(X_test_transformed)\n",
    "\n",
    "# Printing classification report\n",
    "print(classification_report(y_test, y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
